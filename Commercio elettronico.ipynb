{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commercio elettronico\n",
    "Corso seguito nell'anno 2014-2015 a Ca'Foscari dal professor Claudio Silvestri\n",
    "\n",
    "## Prima esercitazione\n",
    "Lo scopo di questa esercitazione è la creazione di una collezione di documenti di testo a partire da un insieme di pagine web di interesse.\n",
    "\n",
    "* scegliere un sito web\n",
    "\n",
    "* ricerca su Google limitata ad un sito (es: \"site:www.ansa.it parlamento\")\n",
    "\n",
    "* parsing dei risultati per trovare gli url delle pagine trovate (attenzione: ci sono anche altri link da non considerare come quelli alla copia in cache). Suggerimento: disabilitare Javascript sul browser e guardare il contenuto del risultato di una ricerca\n",
    "\n",
    "* get delle pagine individuate (salvare su file e non scaricare nuovamente se esiste già)\n",
    "\n",
    "* parsing e salvataggio di testo ed eventuali attributi delle pagine\n",
    "\n",
    "* creare il lessico, aggiornare un contatore delle occorrenze di ciascun termine (totali e numero di documenti distinti). Ordinare per frequenza descrescente, esportare i dati e tracciare dei grafici (x:rank, y: frequenza) utilizzando sia scale lineari che logaritmiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import search\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "SESSION = requests.Session()\n",
    "GOOGLE_SEARCH_STRING = \"site:www.repubblica.it + politica OR crisi OR calcio OR articolo OR article\"\n",
    "NUMERORISULTATI = 120\n",
    "WAITINGTIME = 2  # in secondi\n",
    "QUERYSITO = '//*[@itemprop=\"articleBody\"]/text()'\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "\n",
    "article_url=search(GOOGLE_SEARCH_STRING, stop=NUMERORISULTATI, pause=WAITINGTIME)\n",
    "text=[]\n",
    "for url in article_url:\n",
    "    html = lxml.html.parse(url)\n",
    "    packages = html.xpath(QUERYSITO)\n",
    "    if packages:\n",
    "        text.append(packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seconda esercitazione: content based recommender\n",
    "Content based recommender:\n",
    "\n",
    "* costruire il vector space model per la collezione di testi create nelle scorse lezioni (oppure utilizzare i testi delle notizie Ansa resi disponibili)\n",
    "  * cold start: scegliere un documento e trovare i documenti simili (coseno)\n",
    "  * warm start: scegliere alcuni documenti di vostro interesse e generare dei suggerimenti basati sui contenuti dei documenti scelti  (coseno) \n",
    "\n",
    "###suggerimenti:\n",
    "\n",
    " * utilizzare un array associativo per gestire il lessico ed associare un codice numerico a ciascun termine\n",
    " * il lessico ed il vector space model possono essere salvati e riutilizzati (sono validi fino a che non cambia il contenuto della collezione)\n",
    " \n",
    "###Altre attività (facoltative):\n",
    "\n",
    " * provare con una misura di distanza diversa\n",
    " * utilizzare i contatore delle occorrenze di ciascun termine e numero di documenti per termine per calcolare TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terza esercitazione: MongoDB\n",
    "Preliminari:\n",
    "\n",
    "* installare MongoDB (attenzione: per i PC del laboratorio è necessario specificare la directory da utilizzare per i dati)\n",
    "* fare alcuni inserimenti e ricerche (http://docs.mongodb.org/manual/tutorial/insert-documents/)\n",
    "* installare pymongo (attenzione: per i PC del laboratorio è necessario utilizzare l'opzione --user di pip)\n",
    "* seguire il tutorial pymongo (http://api.mongodb.org/python/current/tutorial.html)\n",
    "* importare i testi delle esercitazioni precedenti (utilizzare pymongo)\n",
    "* creare un indice testuale (http://docs.mongodb.org/manual/core/index-text/, http://docs.mongodb.org/manual/tutorial/create-text-index-on-multiple-fields/) \n",
    "* fare alcune ricerche semplici e composte (http://docs.mongodb.org/manual/reference/operator/query/text/, http://docs.mongodb.org/manual/reference/operator/query/text/#text-operator-text-score)\n",
    "\n",
    "###Esercitazione:\n",
    "\n",
    "* utilizzare map/reduce per trovare le parole più frequenti\n",
    "* utilizzare map reduce per estrarre il lessico, il modello dei documenti (e le posting lists per la parte opzionale) \n",
    "* rappresentare gli utenti, i testi da loro letti, il modello vettoriale del utente\n",
    "* integrazione del text based recommender sviluppato per l'esercitazione 2\n",
    "\n",
    "###Opzionale:\n",
    "\n",
    "* integrazione esercitazione 2bis\n",
    "* utilizzare le posting list per effettuare delle ricerche testuali\n",
    "* confrontare i risultati delle ricerche effettuate utilizzando le posting list e quelli ottenuti utilizzando le funzioni di ricerca testuale di MongoDB\n",
    "* i risultati delle ricerche di MongoDB possono essere  ordinati per  'text score'. Come potremmo implementare una funzionalità simile utilizzando le posting list ed i modelli di documento?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Quarta esercitazione: Twitter\n",
    "\n",
    "* Installare twython (utilizzare \"pip ... --user .... \" in laboratorio) https://github.com/ryanmcgrath/twython\n",
    "* creare un account su Twitter\n",
    "* seguire l'esempio riportato in https://twython.readthedocs.org/en/latest/usage/streaming_api.html\n",
    "* espandere l'esempio filtrando per locazione (v. Twitter streaming API)\n",
    "* stampare periodicamente le 10 parole più frequenti nei Tweet in Veneto (nel rettangolo che contiene il Veneto)\n",
    "* stampare periodicamente le 10 parole più frequenti nei Tweet che contengono la parola Venezia/Venice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}