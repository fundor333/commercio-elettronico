{
 "metadata": {
  "name": "",
  "signature": "sha256:59edb3335e15fcd2aad2f22ea50dac6d83647b0673e10ac18cc08af741040f40"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Commercio elettronico\n",
      "Corso seguito nell'anno 2014-2015 a Ca'Foscari dal professor Claudio Silvestri\n",
      "\n",
      "## Prima esercitazione\n",
      "Lo scopo di questa esercitazione \u00e8 la creazione di una collezione di documenti di testo a partire da un insieme di pagine web di interesse.\n",
      "\n",
      "* scegliere un sito web\n",
      "\n",
      "* ricerca su Google limitata ad un sito (es: \"site:www.ansa.it parlamento\")\n",
      "\n",
      "* parsing dei risultati per trovare gli url delle pagine trovate (attenzione: ci sono anche altri link da non considerare come quelli alla copia in cache). Suggerimento: disabilitare Javascript sul browser e guardare il contenuto del risultato di una ricerca\n",
      "\n",
      "* get delle pagine individuate (salvare su file e non scaricare nuovamente se esiste gi\u00e0)\n",
      "\n",
      "* parsing e salvataggio di testo ed eventuali attributi delle pagine\n",
      "\n",
      "* creare il lessico, aggiornare un contatore delle occorrenze di ciascun termine (totali e numero di documenti distinti). Ordinare per frequenza descrescente, esportare i dati e tracciare dei grafici (x:rank, y: frequenza) utilizzando sia scale lineari che logaritmiche."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import codecs\n",
      "import urllib\n",
      "import time\n",
      "\n",
      "import requests\n",
      "import lxml.html as html\n",
      "import BeautifulSoup\n",
      "\n",
      "\n",
      "SESSION = requests.Session()\n",
      "GOOGLEURL = \"https://www.google.it/search?q=site:www.repubblica.it+%2B+politica+OR+crisi+OR+calcio+OR+articolo+OR+articoli&tbm=nws&num=100&start=\"\n",
      "OUTPITFILENAME = \"out\"\n",
      "NUMERORISULTATI = 100\n",
      "WAITINGTIME = 3  # in secondi\n",
      "QUERYGOOGLE = '//h3[@class=\"r\"]/a/@href'\n",
      "QUERYSITO = '//*[@itemprop=\"articleBody\"]/text()'\n",
      "\n",
      "class AppURLopener(urllib.FancyURLopener):\n",
      "    version = \"App/1.7\"\n",
      "\n",
      "def decode_html(nome):\n",
      "    html_string = file(str(nome) + '.html').read()\n",
      "    converted = BeautifulSoup.UnicodeDammit(html_string, isHTML=True)\n",
      "    if not converted.unicode:\n",
      "        print(\"Errore conversione unicode\")\n",
      "        return ''\n",
      "    return converted.unicode\n",
      "\n",
      "def linkgetter(urlpage, waiting):\n",
      "    urldictionary = []\n",
      "    serverresponce = SESSION.get(urlpage)\n",
      "    time.sleep(waiting)\n",
      "    queryresult = html.fromstring(serverresponce.text).xpath(QUERYGOOGLE)\n",
      "    for url in queryresult:\n",
      "        if url.split(\"/search?q=\")[0] != \"\":\n",
      "            url = url.split(\"/url?q=\")[1]\n",
      "            url = url.split('&sa=')[0]\n",
      "            urldictionary.append(url)\n",
      "    return urldictionary\n",
      "\n",
      "def getarticle(url, number):\n",
      "    if url == \"\":\n",
      "        return number\n",
      "    else:\n",
      "        urllib.urlretrieve(url, \"./html/\" + str(number) + '.html')\n",
      "        urlhtml = html.fromstring(decode_html(\"./html/\" + str(number)))\n",
      "        articlebody = urlhtml.xpath(QUERYSITO)\n",
      "        if articlebody == []:\n",
      "            return number\n",
      "        else:\n",
      "            fileout = codecs.open('./out/' + str(number) + '.txt', 'w', 'utf-8')\n",
      "            reference = \"\"\n",
      "            stopword = open(\"spamword.teo\")\n",
      "            stoplist = None\n",
      "            for line in stopword:\n",
      "                stoplist = set(line.split())\n",
      "            for parolanonelaborata in str(articlebody).split():\n",
      "                for singolaparola in re.split(\"[^a-zA-Z]\", parolanonelaborata):\n",
      "                    if singolaparola != '' or singolaparola not in stoplist:\n",
      "                        reference = reference + \" \" + singolaparola.lower()\n",
      "\n",
      "            print >> fileout, reference\n",
      "            fileout.close()\n",
      "            return number + 1\n",
      "        \n",
      "def getfromgoogle(numberpages):\n",
      "    urddictionary = {}\n",
      "\n",
      "    for i in range(0, numberpages):\n",
      "        print(\"From Google n \" + str(i) + \" out of \" + str(NUMERORISULTATI))\n",
      "        for element in linkgetter(GOOGLEURL + str(i * 10), WAITINGTIME):\n",
      "            urddictionary[element] = i\n",
      "    outdictionary = {}\n",
      "    num = 0\n",
      "\n",
      "    for url in urddictionary.keys():\n",
      "        print(\"From Url n \" + str(num))\n",
      "        num = getarticle(url, num)\n",
      "    maindict = {}\n",
      "\n",
      "    for i in range(0, num - 1):\n",
      "        filein = open(\"./out/\" + str(i) + '.txt')\n",
      "        refline = \"\"\n",
      "        for line in filein:\n",
      "            refline += line\n",
      "        outdictionary[i] = refline\n",
      "    numberfile = i\n",
      "\n",
      "    for text in outdictionary.items():\n",
      "        maindict = adddictionary(maindict, getsingledict(text[1]))\n",
      "    printdict(maindict, OUTPITFILENAME, numberfile)\n",
      "    return numberfile\n",
      "\n",
      "def adddictionary(maindict, secondarydictionary):\n",
      "    for element in secondarydictionary:\n",
      "        if element in maindict.keys():\n",
      "            maindict[element] = (\n",
      "                maindict[element][0] + 1, maindict[element][1] + secondarydictionary[element])\n",
      "        else:\n",
      "            maindict[element] = (1, secondarydictionary[element])\n",
      "    return maindict\n",
      "\n",
      "\n",
      "def getsingledict(imputtext):\n",
      "    dictionary = {}\n",
      "    for word in imputtext.split():\n",
      "        if word in dictionary.keys():\n",
      "            dictionary[word] += 1\n",
      "        else:\n",
      "            dictionary[word] = 1\n",
      "    return dictionary\n",
      "\n",
      "\n",
      "def printdict(dictionary, filename, number):\n",
      "    fileout = codecs.open('./out/' + filename + '.txt', 'w', 'utf-8')\n",
      "    fileout.write(str(number) + '\\n')\n",
      "    for key in dictionary:\n",
      "        fileout.write(key + \" \" + str(dictionary[key][0]) + \" \" + str(dictionary[key][1]) + '\\n')\n",
      "    fileout.close()\n",
      "    \n",
      "    \n",
      "urllib._urlopener = AppURLopener()\n",
      "getfromgoogle(NUMERORISULTATI)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Seconda esercitazione: content based recommender\n",
      "Content based recommender:\n",
      "\n",
      "* costruire il vector space model per la collezione di testi create nelle scorse lezioni (oppure utilizzare i testi delle notizie Ansa resi disponibili)\n",
      "  * cold start: scegliere un documento e trovare i documenti simili (coseno)\n",
      "  * warm start: scegliere alcuni documenti di vostro interesse e generare dei suggerimenti basati sui contenuti dei documenti scelti  (coseno) \n",
      "\n",
      "###suggerimenti:\n",
      "\n",
      " * utilizzare un array associativo per gestire il lessico ed associare un codice numerico a ciascun termine\n",
      " * il lessico ed il vector space model possono essere salvati e riutilizzati (sono validi fino a che non cambia il contenuto della collezione)\n",
      " \n",
      "###Altre attivit\u00e0 (facoltative):\n",
      "\n",
      " * provare con una misura di distanza diversa\n",
      " * utilizzare i contatore delle occorrenze di ciascun termine e numero di documenti per termine per calcolare TF/IDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Terza esercitazione: MongoDB\n",
      "Preliminari:\n",
      "\n",
      "* installare MongoDB (attenzione: per i PC del laboratorio \u00e8 necessario specificare la directory da utilizzare per i dati)\n",
      "* fare alcuni inserimenti e ricerche (http://docs.mongodb.org/manual/tutorial/insert-documents/)\n",
      "* installare pymongo (attenzione: per i PC del laboratorio \u00e8 necessario utilizzare l'opzione --user di pip)\n",
      "* seguire il tutorial pymongo (http://api.mongodb.org/python/current/tutorial.html)\n",
      "* importare i testi delle esercitazioni precedenti (utilizzare pymongo)\n",
      "* creare un indice testuale (http://docs.mongodb.org/manual/core/index-text/, http://docs.mongodb.org/manual/tutorial/create-text-index-on-multiple-fields/) \n",
      "* fare alcune ricerche semplici e composte (http://docs.mongodb.org/manual/reference/operator/query/text/, http://docs.mongodb.org/manual/reference/operator/query/text/#text-operator-text-score)\n",
      "\n",
      "###Esercitazione:\n",
      "\n",
      "* utilizzare map/reduce per trovare le parole pi\u00f9 frequenti\n",
      "* utilizzare map reduce per estrarre il lessico, il modello dei documenti (e le posting lists per la parte opzionale) \n",
      "* rappresentare gli utenti, i testi da loro letti, il modello vettoriale del utente\n",
      "* integrazione del text based recommender sviluppato per l'esercitazione 2\n",
      "\n",
      "###Opzionale:\n",
      "\n",
      "* integrazione esercitazione 2bis\n",
      "* utilizzare le posting list per effettuare delle ricerche testuali\n",
      "* confrontare i risultati delle ricerche effettuate utilizzando le posting list e quelli ottenuti utilizzando le funzioni di ricerca testuale di MongoDB\n",
      "* i risultati delle ricerche di MongoDB possono essere  ordinati per  'text score'. Come potremmo implementare una funzionalit\u00e0 simile utilizzando le posting list ed i modelli di documento?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quarta esercitazione: Twitter\n",
      "\n",
      "* Installare twython (utilizzare \"pip ... --user .... \" in laboratorio) https://github.com/ryanmcgrath/twython\n",
      "* creare un account su Twitter\n",
      "* seguire l'esempio riportato in https://twython.readthedocs.org/en/latest/usage/streaming_api.html\n",
      "* espandere l'esempio filtrando per locazione (v. Twitter streaming API)\n",
      "* stampare periodicamente le 10 parole pi\u00f9 frequenti nei Tweet in Veneto (nel rettangolo che contiene il Veneto)\n",
      "* stampare periodicamente le 10 parole pi\u00f9 frequenti nei Tweet che contengono la parola Venezia/Venice\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}